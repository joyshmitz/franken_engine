//! Storage adapter boundary for deterministic FrankenEngine persistence paths.
//!
//! This module introduces a thin adapter layer between runtime persistence
//! contracts and `/dp/frankensqlite` backends. The interface is intentionally
//! store-agnostic and deterministic:
//! - stable query ordering
//! - explicit schema version checks and migration receipts
//! - structured operation events with canonical logging fields
//!
//! Plan references: Section 10.14 item 6 (`bd-89l2`), ADR-0004.

use std::collections::BTreeMap;
use std::fmt;

use serde::{Deserialize, Serialize};

/// Current schema version for storage-adapter contracts.
pub const STORAGE_SCHEMA_VERSION: u32 = 1;

/// Canonical control-plane stores mapped in the persistence inventory.
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash, Serialize, Deserialize)]
pub enum StoreKind {
    ReplayIndex,
    EvidenceIndex,
    BenchmarkLedger,
    PolicyCache,
    PlasWitness,
    ReplacementLineage,
    IfcProvenance,
    SpecializationIndex,
}

impl StoreKind {
    /// Stable string name used in logs and deterministic serialization.
    pub fn as_str(self) -> &'static str {
        match self {
            Self::ReplayIndex => "replay_index",
            Self::EvidenceIndex => "evidence_index",
            Self::BenchmarkLedger => "benchmark_ledger",
            Self::PolicyCache => "policy_cache",
            Self::PlasWitness => "plas_witness",
            Self::ReplacementLineage => "replacement_lineage",
            Self::IfcProvenance => "ifc_provenance",
            Self::SpecializationIndex => "specialization_index",
        }
    }

    /// Inventory-mapped frankensqlite integration point for the store.
    pub fn integration_point(self) -> &'static str {
        match self {
            Self::ReplayIndex => "frankensqlite::control_plane::replay_index",
            Self::EvidenceIndex => "frankensqlite::control_plane::evidence_index",
            Self::BenchmarkLedger => "frankensqlite::benchmark::ledger",
            Self::PolicyCache => "frankensqlite::control_plane::policy_cache",
            Self::PlasWitness => "frankensqlite::analysis::plas_witness",
            Self::ReplacementLineage => "frankensqlite::replacement::lineage_log",
            Self::IfcProvenance => "frankensqlite::control_plane::ifc_provenance",
            Self::SpecializationIndex => "frankensqlite::control_plane::specialization_index",
        }
    }
}

impl fmt::Display for StoreKind {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.write_str(self.as_str())
    }
}

/// Canonical context carried into adapter operations.
///
/// Field names intentionally match required structured log keys.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct EventContext {
    pub trace_id: String,
    pub decision_id: String,
    pub policy_id: String,
}

impl EventContext {
    /// Build a validated operation context.
    pub fn new(
        trace_id: impl Into<String>,
        decision_id: impl Into<String>,
        policy_id: impl Into<String>,
    ) -> Result<Self, StorageError> {
        let trace_id = trace_id.into();
        let decision_id = decision_id.into();
        let policy_id = policy_id.into();
        if trace_id.trim().is_empty() {
            return Err(StorageError::InvalidContext {
                field: "trace_id".to_string(),
            });
        }
        if decision_id.trim().is_empty() {
            return Err(StorageError::InvalidContext {
                field: "decision_id".to_string(),
            });
        }
        if policy_id.trim().is_empty() {
            return Err(StorageError::InvalidContext {
                field: "policy_id".to_string(),
            });
        }
        Ok(Self {
            trace_id,
            decision_id,
            policy_id,
        })
    }
}

/// Stored value with deterministic metadata.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct StoreRecord {
    pub store: StoreKind,
    pub key: String,
    pub value: Vec<u8>,
    pub metadata: BTreeMap<String, String>,
    pub revision: u64,
}

/// Query selector for deterministic reads.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize, Default)]
pub struct StoreQuery {
    /// Optional key prefix filter.
    pub key_prefix: Option<String>,
    /// Equality filters that must all match.
    pub metadata_filters: BTreeMap<String, String>,
    /// Optional max result size.
    pub limit: Option<usize>,
}

/// Batched write entry.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct BatchPutEntry {
    pub key: String,
    pub value: Vec<u8>,
    pub metadata: BTreeMap<String, String>,
}

/// Deterministic migration receipt.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct MigrationReceipt {
    pub backend: String,
    pub from_version: u32,
    pub to_version: u32,
    pub stores_touched: Vec<StoreKind>,
    pub records_touched: u64,
    pub state_hash_before: String,
    pub state_hash_after: String,
}

/// Canonical structured event emitted by adapter operations.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct StorageEvent {
    pub trace_id: String,
    pub decision_id: String,
    pub policy_id: String,
    pub component: String,
    pub event: String,
    pub outcome: String,
    pub error_code: Option<String>,
}

/// Stable error taxonomy for storage operations.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub enum StorageError {
    InvalidContext { field: String },
    InvalidKey { key: String },
    InvalidQuery { detail: String },
    NotFound { store: StoreKind, key: String },
    SchemaVersionMismatch { expected: u32, actual: u32 },
    MigrationFailed { from: u32, to: u32, reason: String },
    IntegrityViolation { store: StoreKind, detail: String },
    BackendUnavailable { backend: String, detail: String },
    WriteRejected { detail: String },
}

impl StorageError {
    /// Stable machine-readable error code.
    pub fn code(&self) -> &'static str {
        match self {
            Self::InvalidContext { .. } => "FE-STOR-0001",
            Self::InvalidKey { .. } => "FE-STOR-0002",
            Self::InvalidQuery { .. } => "FE-STOR-0003",
            Self::NotFound { .. } => "FE-STOR-0004",
            Self::SchemaVersionMismatch { .. } => "FE-STOR-0005",
            Self::MigrationFailed { .. } => "FE-STOR-0006",
            Self::IntegrityViolation { .. } => "FE-STOR-0007",
            Self::BackendUnavailable { .. } => "FE-STOR-0008",
            Self::WriteRejected { .. } => "FE-STOR-0009",
        }
    }
}

impl fmt::Display for StorageError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::InvalidContext { field } => write!(f, "invalid context field: {field}"),
            Self::InvalidKey { key } => write!(f, "invalid key: `{key}`"),
            Self::InvalidQuery { detail } => write!(f, "invalid query: {detail}"),
            Self::NotFound { store, key } => write!(f, "record not found: {store}/{key}"),
            Self::SchemaVersionMismatch { expected, actual } => {
                write!(
                    f,
                    "schema version mismatch: expected {expected}, got {actual}"
                )
            }
            Self::MigrationFailed { from, to, reason } => {
                write!(f, "migration failed: {from} -> {to}: {reason}")
            }
            Self::IntegrityViolation { store, detail } => {
                write!(f, "integrity violation in {store}: {detail}")
            }
            Self::BackendUnavailable { backend, detail } => {
                write!(f, "backend unavailable ({backend}): {detail}")
            }
            Self::WriteRejected { detail } => write!(f, "write rejected: {detail}"),
        }
    }
}

impl std::error::Error for StorageError {}

/// Generic storage adapter contract.
pub trait StorageAdapter {
    /// Adapter backend identifier.
    fn backend_name(&self) -> &'static str;
    /// Current schema version.
    fn current_schema_version(&self) -> u32;
    /// Fail-closed schema check for callers requiring a specific version.
    fn ensure_schema_version(&self, expected: u32) -> Result<(), StorageError>;
    /// Apply deterministic schema migration.
    fn migrate_to(&mut self, target_version: u32) -> Result<MigrationReceipt, StorageError>;

    fn put(
        &mut self,
        store: StoreKind,
        key: String,
        value: Vec<u8>,
        metadata: BTreeMap<String, String>,
        context: &EventContext,
    ) -> Result<StoreRecord, StorageError>;

    fn get(
        &mut self,
        store: StoreKind,
        key: &str,
        context: &EventContext,
    ) -> Result<Option<StoreRecord>, StorageError>;

    fn query(
        &mut self,
        store: StoreKind,
        query: &StoreQuery,
        context: &EventContext,
    ) -> Result<Vec<StoreRecord>, StorageError>;

    fn delete(
        &mut self,
        store: StoreKind,
        key: &str,
        context: &EventContext,
    ) -> Result<bool, StorageError>;

    fn put_batch(
        &mut self,
        store: StoreKind,
        entries: Vec<BatchPutEntry>,
        context: &EventContext,
    ) -> Result<Vec<StoreRecord>, StorageError>;

    fn events(&self) -> &[StorageEvent];
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
struct StoreState {
    next_revision: u64,
    records: BTreeMap<String, StoreRecord>,
}

impl StoreState {
    fn put(
        &mut self,
        store: StoreKind,
        key: String,
        value: Vec<u8>,
        metadata: BTreeMap<String, String>,
    ) -> StoreRecord {
        self.next_revision = self.next_revision.saturating_add(1);
        let record = StoreRecord {
            store,
            key: key.clone(),
            value,
            metadata,
            revision: self.next_revision,
        };
        self.records.insert(key, record.clone());
        record
    }
}

/// Deterministic in-memory adapter used for tests and local workflows.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InMemoryStorageAdapter {
    schema_version: u32,
    stores: BTreeMap<StoreKind, StoreState>,
    events: Vec<StorageEvent>,
    fail_writes: bool,
}

impl Default for InMemoryStorageAdapter {
    fn default() -> Self {
        Self {
            schema_version: STORAGE_SCHEMA_VERSION,
            stores: BTreeMap::new(),
            events: Vec::new(),
            fail_writes: false,
        }
    }
}

impl InMemoryStorageAdapter {
    pub fn new() -> Self {
        Self::default()
    }

    /// Optional failure-injection mode for tests.
    pub fn with_fail_writes(mut self, fail_writes: bool) -> Self {
        self.fail_writes = fail_writes;
        self
    }

    fn validate_key(key: &str) -> Result<(), StorageError> {
        if key.trim().is_empty() {
            return Err(StorageError::InvalidKey {
                key: key.to_string(),
            });
        }
        Ok(())
    }

    fn get_or_insert_state(&mut self, store: StoreKind) -> &mut StoreState {
        self.stores.entry(store).or_default()
    }

    fn record_event(
        &mut self,
        context: &EventContext,
        event: &str,
        outcome: &str,
        error: Option<&StorageError>,
    ) {
        self.events.push(StorageEvent {
            trace_id: context.trace_id.clone(),
            decision_id: context.decision_id.clone(),
            policy_id: context.policy_id.clone(),
            component: "storage_adapter".to_string(),
            event: event.to_string(),
            outcome: outcome.to_string(),
            error_code: error.map(|err| err.code().to_string()),
        });
    }

    fn state_hash(&self) -> String {
        let bytes = serde_json::to_vec(&(self.schema_version, &self.stores)).unwrap_or_default();
        digest_hex(&bytes)
    }

    fn total_records(&self) -> u64 {
        self.stores
            .values()
            .map(|state| state.records.len() as u64)
            .sum()
    }

    pub fn events(&self) -> &[StorageEvent] {
        &self.events
    }
}

impl StorageAdapter for InMemoryStorageAdapter {
    fn backend_name(&self) -> &'static str {
        "in_memory"
    }

    fn current_schema_version(&self) -> u32 {
        self.schema_version
    }

    fn ensure_schema_version(&self, expected: u32) -> Result<(), StorageError> {
        if self.schema_version == expected {
            Ok(())
        } else {
            Err(StorageError::SchemaVersionMismatch {
                expected,
                actual: self.schema_version,
            })
        }
    }

    fn migrate_to(&mut self, target_version: u32) -> Result<MigrationReceipt, StorageError> {
        if target_version < self.schema_version {
            return Err(StorageError::MigrationFailed {
                from: self.schema_version,
                to: target_version,
                reason: "downgrade is not supported".to_string(),
            });
        }
        if target_version > self.schema_version.saturating_add(1) {
            return Err(StorageError::MigrationFailed {
                from: self.schema_version,
                to: target_version,
                reason: "only single-step migrations are allowed".to_string(),
            });
        }

        let from_version = self.schema_version;
        let state_hash_before = self.state_hash();
        self.schema_version = target_version;
        let state_hash_after = self.state_hash();
        let stores_touched = self.stores.keys().copied().collect();

        Ok(MigrationReceipt {
            backend: self.backend_name().to_string(),
            from_version,
            to_version: target_version,
            stores_touched,
            records_touched: self.total_records(),
            state_hash_before,
            state_hash_after,
        })
    }

    fn put(
        &mut self,
        store: StoreKind,
        key: String,
        value: Vec<u8>,
        metadata: BTreeMap<String, String>,
        context: &EventContext,
    ) -> Result<StoreRecord, StorageError> {
        let result = (|| {
            if self.fail_writes {
                return Err(StorageError::WriteRejected {
                    detail: "write failure injected".to_string(),
                });
            }
            Self::validate_key(&key)?;
            Ok(self
                .get_or_insert_state(store)
                .put(store, key, value, metadata))
        })();

        self.record_event(
            context,
            "put",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn get(
        &mut self,
        store: StoreKind,
        key: &str,
        context: &EventContext,
    ) -> Result<Option<StoreRecord>, StorageError> {
        let result = (|| {
            Self::validate_key(key)?;
            Ok(self
                .stores
                .get(&store)
                .and_then(|state| state.records.get(key).cloned()))
        })();

        self.record_event(
            context,
            "get",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn query(
        &mut self,
        store: StoreKind,
        query: &StoreQuery,
        context: &EventContext,
    ) -> Result<Vec<StoreRecord>, StorageError> {
        let result = (|| {
            if matches!(query.limit, Some(0)) {
                return Err(StorageError::InvalidQuery {
                    detail: "limit cannot be zero".to_string(),
                });
            }

            let Some(state) = self.stores.get(&store) else {
                return Ok(Vec::new());
            };

            let out: Vec<StoreRecord> = state
                .records
                .values()
                .filter(|record| {
                    if let Some(prefix) = &query.key_prefix
                        && !record.key.starts_with(prefix)
                    {
                        return false;
                    }
                    query
                        .metadata_filters
                        .iter()
                        .all(|(k, v)| record.metadata.get(k) == Some(v))
                })
                .cloned()
                .collect();

            Ok(canonicalize_records(out, query.limit))
        })();

        self.record_event(
            context,
            "query",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn delete(
        &mut self,
        store: StoreKind,
        key: &str,
        context: &EventContext,
    ) -> Result<bool, StorageError> {
        let result = (|| {
            if self.fail_writes {
                return Err(StorageError::WriteRejected {
                    detail: "write failure injected".to_string(),
                });
            }
            Self::validate_key(key)?;
            Ok(self
                .stores
                .get_mut(&store)
                .and_then(|state| state.records.remove(key))
                .is_some())
        })();

        self.record_event(
            context,
            "delete",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn put_batch(
        &mut self,
        store: StoreKind,
        entries: Vec<BatchPutEntry>,
        context: &EventContext,
    ) -> Result<Vec<StoreRecord>, StorageError> {
        let result = (|| {
            if self.fail_writes {
                return Err(StorageError::WriteRejected {
                    detail: "write failure injected".to_string(),
                });
            }
            let mut staged = self.stores.get(&store).cloned().unwrap_or_default();
            let mut out = Vec::with_capacity(entries.len());
            for entry in entries {
                Self::validate_key(&entry.key)?;
                out.push(staged.put(store, entry.key, entry.value, entry.metadata));
            }
            self.stores.insert(store, staged);
            Ok(out)
        })();

        self.record_event(
            context,
            "put_batch",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn events(&self) -> &[StorageEvent] {
        self.events()
    }
}

/// Minimal backend contract expected from `/dp/frankensqlite` integration.
///
/// This seam lets `franken_engine` depend on stable adapter behavior without
/// owning WAL/PRAGMA/migration internals locally.
pub trait FrankensqliteBackend {
    fn apply_wal_profile(&mut self) -> Result<(), String>;
    fn set_pragma(&mut self, key: &str, value: &str) -> Result<(), String>;
    fn current_schema_version(&self) -> Result<u32, String>;
    fn migrate_to(&mut self, target_version: u32) -> Result<(), String>;
    fn put_record(
        &mut self,
        store: StoreKind,
        key: &str,
        value: &[u8],
        metadata: &BTreeMap<String, String>,
    ) -> Result<StoreRecord, String>;
    fn get_record(&self, store: StoreKind, key: &str) -> Result<Option<StoreRecord>, String>;
    fn query_records(
        &self,
        store: StoreKind,
        query: &StoreQuery,
    ) -> Result<Vec<StoreRecord>, String>;
    fn delete_record(&mut self, store: StoreKind, key: &str) -> Result<bool, String>;
    fn put_batch(
        &mut self,
        store: StoreKind,
        entries: &[BatchPutEntry],
    ) -> Result<Vec<StoreRecord>, String>;
}

/// Adapter implementation backed by a frankensqlite integration backend.
#[derive(Debug)]
pub struct FrankensqliteStorageAdapter<B: FrankensqliteBackend> {
    backend: B,
    schema_version: u32,
    events: Vec<StorageEvent>,
}

impl<B: FrankensqliteBackend> FrankensqliteStorageAdapter<B> {
    pub fn new(mut backend: B) -> Result<Self, StorageError> {
        backend
            .apply_wal_profile()
            .map_err(|detail| StorageError::BackendUnavailable {
                backend: "frankensqlite".to_string(),
                detail,
            })?;

        // These defaults match ADR-0004 posture; frankensqlite owns specifics.
        for (key, value) in [
            ("journal_mode", "WAL"),
            ("busy_timeout", "5000"),
            ("foreign_keys", "ON"),
        ] {
            backend
                .set_pragma(key, value)
                .map_err(|detail| StorageError::BackendUnavailable {
                    backend: "frankensqlite".to_string(),
                    detail,
                })?;
        }

        let schema_version = backend.current_schema_version().map_err(|detail| {
            StorageError::BackendUnavailable {
                backend: "frankensqlite".to_string(),
                detail,
            }
        })?;

        Ok(Self {
            backend,
            schema_version,
            events: Vec::new(),
        })
    }

    fn map_backend_error(detail: String) -> StorageError {
        StorageError::BackendUnavailable {
            backend: "frankensqlite".to_string(),
            detail,
        }
    }

    fn record_event(
        &mut self,
        context: &EventContext,
        event: &str,
        outcome: &str,
        error: Option<&StorageError>,
    ) {
        self.events.push(StorageEvent {
            trace_id: context.trace_id.clone(),
            decision_id: context.decision_id.clone(),
            policy_id: context.policy_id.clone(),
            component: "storage_adapter".to_string(),
            event: event.to_string(),
            outcome: outcome.to_string(),
            error_code: error.map(|err| err.code().to_string()),
        });
    }
}

impl<B: FrankensqliteBackend> StorageAdapter for FrankensqliteStorageAdapter<B> {
    fn backend_name(&self) -> &'static str {
        "frankensqlite"
    }

    fn current_schema_version(&self) -> u32 {
        self.schema_version
    }

    fn ensure_schema_version(&self, expected: u32) -> Result<(), StorageError> {
        if self.schema_version == expected {
            Ok(())
        } else {
            Err(StorageError::SchemaVersionMismatch {
                expected,
                actual: self.schema_version,
            })
        }
    }

    fn migrate_to(&mut self, target_version: u32) -> Result<MigrationReceipt, StorageError> {
        if target_version < self.schema_version {
            return Err(StorageError::MigrationFailed {
                from: self.schema_version,
                to: target_version,
                reason: "downgrade is not supported".to_string(),
            });
        }
        if target_version > self.schema_version.saturating_add(1) {
            return Err(StorageError::MigrationFailed {
                from: self.schema_version,
                to: target_version,
                reason: "only single-step migrations are allowed".to_string(),
            });
        }

        let from_version = self.schema_version;
        let state_hash_before = digest_hex(format!("schema:{from_version}").as_bytes());
        self.backend
            .migrate_to(target_version)
            .map_err(Self::map_backend_error)?;
        self.schema_version = target_version;
        let state_hash_after = digest_hex(format!("schema:{target_version}").as_bytes());

        Ok(MigrationReceipt {
            backend: self.backend_name().to_string(),
            from_version,
            to_version: target_version,
            stores_touched: Vec::new(),
            records_touched: 0,
            state_hash_before,
            state_hash_after,
        })
    }

    fn put(
        &mut self,
        store: StoreKind,
        key: String,
        value: Vec<u8>,
        metadata: BTreeMap<String, String>,
        context: &EventContext,
    ) -> Result<StoreRecord, StorageError> {
        let result = (|| {
            InMemoryStorageAdapter::validate_key(&key)?;
            self.backend
                .put_record(store, &key, &value, &metadata)
                .map_err(Self::map_backend_error)
        })();

        self.record_event(
            context,
            "put",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn get(
        &mut self,
        store: StoreKind,
        key: &str,
        context: &EventContext,
    ) -> Result<Option<StoreRecord>, StorageError> {
        let result = (|| {
            InMemoryStorageAdapter::validate_key(key)?;
            self.backend
                .get_record(store, key)
                .map_err(Self::map_backend_error)
        })();

        self.record_event(
            context,
            "get",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn query(
        &mut self,
        store: StoreKind,
        query: &StoreQuery,
        context: &EventContext,
    ) -> Result<Vec<StoreRecord>, StorageError> {
        let result = (|| {
            if matches!(query.limit, Some(0)) {
                return Err(StorageError::InvalidQuery {
                    detail: "limit cannot be zero".to_string(),
                });
            }

            // Query without a limit first, then canonicalize and truncate locally.
            // This prevents backend row-order variation from changing visible results.
            let mut unconstrained = query.clone();
            unconstrained.limit = None;

            let rows = self
                .backend
                .query_records(store, &unconstrained)
                .map_err(Self::map_backend_error)?;
            Ok(canonicalize_records(rows, query.limit))
        })();

        self.record_event(
            context,
            "query",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn delete(
        &mut self,
        store: StoreKind,
        key: &str,
        context: &EventContext,
    ) -> Result<bool, StorageError> {
        let result = (|| {
            InMemoryStorageAdapter::validate_key(key)?;
            self.backend
                .delete_record(store, key)
                .map_err(Self::map_backend_error)
        })();

        self.record_event(
            context,
            "delete",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn put_batch(
        &mut self,
        store: StoreKind,
        entries: Vec<BatchPutEntry>,
        context: &EventContext,
    ) -> Result<Vec<StoreRecord>, StorageError> {
        let result = (|| {
            for entry in &entries {
                InMemoryStorageAdapter::validate_key(&entry.key)?;
            }
            self.backend
                .put_batch(store, &entries)
                .map_err(Self::map_backend_error)
        })();

        self.record_event(
            context,
            "put_batch",
            if result.is_ok() { "ok" } else { "error" },
            result.as_ref().err(),
        );
        result
    }

    fn events(&self) -> &[StorageEvent] {
        &self.events
    }
}

fn digest_hex(bytes: &[u8]) -> String {
    format!("{:016x}", fnv1a64(bytes))
}

fn canonicalize_records(mut rows: Vec<StoreRecord>, limit: Option<usize>) -> Vec<StoreRecord> {
    rows.sort_by(|a, b| {
        a.key
            .cmp(&b.key)
            .then(a.revision.cmp(&b.revision))
            .then(a.value.cmp(&b.value))
            .then(a.metadata.cmp(&b.metadata))
    });

    if let Some(limit) = limit {
        rows.truncate(limit);
    }
    rows
}

fn fnv1a64(bytes: &[u8]) -> u64 {
    const OFFSET: u64 = 0xcbf2_9ce4_8422_2325;
    const PRIME: u64 = 0x0100_0000_01b3;

    let mut hash = OFFSET;
    for byte in bytes {
        hash ^= u64::from(*byte);
        hash = hash.wrapping_mul(PRIME);
    }
    hash
}

#[cfg(test)]
mod tests {
    use super::*;

    fn ctx() -> EventContext {
        EventContext::new("trace-storage", "decision-storage", "policy-storage")
            .expect("context should be valid")
    }

    #[test]
    fn in_memory_adapter_crud_and_query_are_deterministic() {
        let mut adapter = InMemoryStorageAdapter::new();
        let context = ctx();

        let mut meta_a = BTreeMap::new();
        meta_a.insert("kind".to_string(), "benchmark".to_string());
        let mut meta_b = BTreeMap::new();
        meta_b.insert("kind".to_string(), "benchmark".to_string());

        adapter
            .put(
                StoreKind::BenchmarkLedger,
                "bench/z".to_string(),
                vec![2],
                meta_a,
                &context,
            )
            .expect("put z");
        adapter
            .put(
                StoreKind::BenchmarkLedger,
                "bench/a".to_string(),
                vec![1],
                meta_b,
                &context,
            )
            .expect("put a");

        let rows = adapter
            .query(StoreKind::BenchmarkLedger, &StoreQuery::default(), &context)
            .expect("query");
        assert_eq!(rows.len(), 2);
        assert_eq!(rows[0].key, "bench/a");
        assert_eq!(rows[1].key, "bench/z");

        let loaded = adapter
            .get(StoreKind::BenchmarkLedger, "bench/a", &context)
            .expect("get")
            .expect("must exist");
        assert_eq!(loaded.value, vec![1]);

        assert!(
            adapter
                .delete(StoreKind::BenchmarkLedger, "bench/z", &context)
                .expect("delete")
        );
    }

    #[test]
    fn in_memory_batch_put_is_atomic_on_invalid_key() {
        let mut adapter = InMemoryStorageAdapter::new();
        let context = ctx();

        adapter
            .put(
                StoreKind::ReplayIndex,
                "run/seed".to_string(),
                vec![9],
                BTreeMap::new(),
                &context,
            )
            .expect("seed row");

        let bad_batch = vec![
            BatchPutEntry {
                key: "run/1".to_string(),
                value: vec![1],
                metadata: BTreeMap::new(),
            },
            BatchPutEntry {
                key: "   ".to_string(),
                value: vec![2],
                metadata: BTreeMap::new(),
            },
        ];

        let err = adapter
            .put_batch(StoreKind::ReplayIndex, bad_batch, &context)
            .expect_err("batch should fail");
        assert!(matches!(err, StorageError::InvalidKey { .. }));

        let rows = adapter
            .query(StoreKind::ReplayIndex, &StoreQuery::default(), &context)
            .expect("query");
        assert_eq!(rows.len(), 1);
        assert_eq!(rows[0].key, "run/seed");
    }

    #[test]
    fn in_memory_migration_receipt_is_deterministic() {
        let mut adapter = InMemoryStorageAdapter::new();
        let context = ctx();

        adapter
            .put(
                StoreKind::EvidenceIndex,
                "decision/1".to_string(),
                vec![7, 7],
                BTreeMap::new(),
                &context,
            )
            .expect("put");

        let receipt = adapter
            .migrate_to(STORAGE_SCHEMA_VERSION + 1)
            .expect("migrate");
        assert_eq!(receipt.from_version, STORAGE_SCHEMA_VERSION);
        assert_eq!(receipt.to_version, STORAGE_SCHEMA_VERSION + 1);
        assert_eq!(receipt.records_touched, 1);
        assert_ne!(receipt.state_hash_before, receipt.state_hash_after);
    }

    #[test]
    fn events_include_required_structured_fields() {
        let mut adapter = InMemoryStorageAdapter::new();
        let context = ctx();

        let err = adapter
            .put(
                StoreKind::PolicyCache,
                "".to_string(),
                vec![1],
                BTreeMap::new(),
                &context,
            )
            .expect_err("invalid key must fail");
        assert_eq!(err.code(), "FE-STOR-0002");

        let event = adapter.events().last().expect("event");
        assert_eq!(event.trace_id, "trace-storage");
        assert_eq!(event.decision_id, "decision-storage");
        assert_eq!(event.policy_id, "policy-storage");
        assert_eq!(event.component, "storage_adapter");
        assert_eq!(event.event, "put");
        assert_eq!(event.outcome, "error");
        assert_eq!(event.error_code.as_deref(), Some("FE-STOR-0002"));
    }

    #[derive(Debug, Default)]
    struct MockFrankenSqlite {
        wal_applied: bool,
        pragmas: BTreeMap<String, String>,
        schema_version: u32,
        stores: BTreeMap<StoreKind, StoreState>,
    }

    impl FrankensqliteBackend for MockFrankenSqlite {
        fn apply_wal_profile(&mut self) -> Result<(), String> {
            self.wal_applied = true;
            Ok(())
        }

        fn set_pragma(&mut self, key: &str, value: &str) -> Result<(), String> {
            self.pragmas.insert(key.to_string(), value.to_string());
            Ok(())
        }

        fn current_schema_version(&self) -> Result<u32, String> {
            Ok(self.schema_version.max(STORAGE_SCHEMA_VERSION))
        }

        fn migrate_to(&mut self, target_version: u32) -> Result<(), String> {
            self.schema_version = target_version;
            Ok(())
        }

        fn put_record(
            &mut self,
            store: StoreKind,
            key: &str,
            value: &[u8],
            metadata: &BTreeMap<String, String>,
        ) -> Result<StoreRecord, String> {
            let state = self.stores.entry(store).or_default();
            Ok(state.put(store, key.to_string(), value.to_vec(), metadata.clone()))
        }

        fn get_record(&self, store: StoreKind, key: &str) -> Result<Option<StoreRecord>, String> {
            Ok(self
                .stores
                .get(&store)
                .and_then(|state| state.records.get(key).cloned()))
        }

        fn query_records(
            &self,
            store: StoreKind,
            query: &StoreQuery,
        ) -> Result<Vec<StoreRecord>, String> {
            let mut out = Vec::new();
            if let Some(state) = self.stores.get(&store) {
                for record in state.records.values() {
                    if let Some(prefix) = &query.key_prefix
                        && !record.key.starts_with(prefix)
                    {
                        continue;
                    }
                    if !query
                        .metadata_filters
                        .iter()
                        .all(|(k, v)| record.metadata.get(k) == Some(v))
                    {
                        continue;
                    }
                    out.push(record.clone());
                }
            }
            if let Some(limit) = query.limit {
                out.truncate(limit);
            }
            Ok(out)
        }

        fn delete_record(&mut self, store: StoreKind, key: &str) -> Result<bool, String> {
            Ok(self
                .stores
                .get_mut(&store)
                .and_then(|state| state.records.remove(key))
                .is_some())
        }

        fn put_batch(
            &mut self,
            store: StoreKind,
            entries: &[BatchPutEntry],
        ) -> Result<Vec<StoreRecord>, String> {
            let mut staged = self.stores.get(&store).cloned().unwrap_or_default();
            let mut out = Vec::with_capacity(entries.len());
            for entry in entries {
                out.push(staged.put(
                    store,
                    entry.key.clone(),
                    entry.value.clone(),
                    entry.metadata.clone(),
                ));
            }
            self.stores.insert(store, staged);
            Ok(out)
        }
    }

    #[test]
    fn frankensqlite_adapter_applies_wal_and_pragmas() {
        let backend = MockFrankenSqlite::default();
        let adapter = FrankensqliteStorageAdapter::new(backend).expect("adapter init");
        assert_eq!(adapter.current_schema_version(), STORAGE_SCHEMA_VERSION);
    }

    // ── EventContext validation ──────────────────────────────────────────

    #[test]
    fn event_context_valid() {
        let ctx = EventContext::new("t", "d", "p").unwrap();
        assert_eq!(ctx.trace_id, "t");
        assert_eq!(ctx.decision_id, "d");
        assert_eq!(ctx.policy_id, "p");
    }

    #[test]
    fn event_context_empty_trace_id_errors() {
        let err = EventContext::new("", "d", "p").unwrap_err();
        assert!(matches!(err, StorageError::InvalidContext { field } if field == "trace_id"));
    }

    #[test]
    fn event_context_empty_decision_id_errors() {
        let err = EventContext::new("t", "  ", "p").unwrap_err();
        assert!(matches!(err, StorageError::InvalidContext { field } if field == "decision_id"));
    }

    #[test]
    fn event_context_empty_policy_id_errors() {
        let err = EventContext::new("t", "d", "").unwrap_err();
        assert!(matches!(err, StorageError::InvalidContext { field } if field == "policy_id"));
    }

    // ── StoreKind ────────────────────────────────────────────────────────

    #[test]
    fn store_kind_as_str_exhaustive() {
        let cases = [
            (StoreKind::ReplayIndex, "replay_index"),
            (StoreKind::EvidenceIndex, "evidence_index"),
            (StoreKind::BenchmarkLedger, "benchmark_ledger"),
            (StoreKind::PolicyCache, "policy_cache"),
            (StoreKind::PlasWitness, "plas_witness"),
            (StoreKind::ReplacementLineage, "replacement_lineage"),
            (StoreKind::IfcProvenance, "ifc_provenance"),
            (StoreKind::SpecializationIndex, "specialization_index"),
        ];
        for (kind, expected) in cases {
            assert_eq!(kind.as_str(), expected, "StoreKind::{kind:?}");
        }
    }

    #[test]
    fn store_kind_integration_point_exhaustive() {
        let cases = [
            (
                StoreKind::ReplayIndex,
                "frankensqlite::control_plane::replay_index",
            ),
            (
                StoreKind::EvidenceIndex,
                "frankensqlite::control_plane::evidence_index",
            ),
            (
                StoreKind::BenchmarkLedger,
                "frankensqlite::benchmark::ledger",
            ),
            (
                StoreKind::PolicyCache,
                "frankensqlite::control_plane::policy_cache",
            ),
            (
                StoreKind::PlasWitness,
                "frankensqlite::analysis::plas_witness",
            ),
            (
                StoreKind::ReplacementLineage,
                "frankensqlite::replacement::lineage_log",
            ),
            (
                StoreKind::IfcProvenance,
                "frankensqlite::control_plane::ifc_provenance",
            ),
            (
                StoreKind::SpecializationIndex,
                "frankensqlite::control_plane::specialization_index",
            ),
        ];
        for (kind, expected) in cases {
            assert_eq!(kind.integration_point(), expected, "StoreKind::{kind:?}");
        }
    }

    #[test]
    fn store_kind_display_matches_as_str() {
        for kind in [
            StoreKind::ReplayIndex,
            StoreKind::EvidenceIndex,
            StoreKind::BenchmarkLedger,
            StoreKind::PolicyCache,
            StoreKind::PlasWitness,
            StoreKind::ReplacementLineage,
            StoreKind::IfcProvenance,
            StoreKind::SpecializationIndex,
        ] {
            assert_eq!(format!("{kind}"), kind.as_str());
        }
    }

    #[test]
    fn store_kind_serde_round_trip() {
        let kind = StoreKind::PlasWitness;
        let json = serde_json::to_string(&kind).unwrap();
        let back: StoreKind = serde_json::from_str(&json).unwrap();
        assert_eq!(back, kind);
    }

    // ── StorageError ─────────────────────────────────────────────────────

    #[test]
    fn storage_error_code_exhaustive() {
        let cases: Vec<(StorageError, &str)> = vec![
            (
                StorageError::InvalidContext { field: "x".into() },
                "FE-STOR-0001",
            ),
            (StorageError::InvalidKey { key: "k".into() }, "FE-STOR-0002"),
            (
                StorageError::InvalidQuery { detail: "d".into() },
                "FE-STOR-0003",
            ),
            (
                StorageError::NotFound {
                    store: StoreKind::PolicyCache,
                    key: "k".into(),
                },
                "FE-STOR-0004",
            ),
            (
                StorageError::SchemaVersionMismatch {
                    expected: 1,
                    actual: 2,
                },
                "FE-STOR-0005",
            ),
            (
                StorageError::MigrationFailed {
                    from: 1,
                    to: 2,
                    reason: "r".into(),
                },
                "FE-STOR-0006",
            ),
            (
                StorageError::IntegrityViolation {
                    store: StoreKind::ReplayIndex,
                    detail: "d".into(),
                },
                "FE-STOR-0007",
            ),
            (
                StorageError::BackendUnavailable {
                    backend: "b".into(),
                    detail: "d".into(),
                },
                "FE-STOR-0008",
            ),
            (
                StorageError::WriteRejected { detail: "d".into() },
                "FE-STOR-0009",
            ),
        ];
        for (err, code) in cases {
            assert_eq!(err.code(), code, "{err}");
        }
    }

    #[test]
    fn storage_error_display_all_variants() {
        let err = StorageError::InvalidContext {
            field: "trace_id".into(),
        };
        assert!(err.to_string().contains("trace_id"));

        let err = StorageError::InvalidKey { key: "bad".into() };
        assert!(err.to_string().contains("bad"));

        let err = StorageError::InvalidQuery {
            detail: "oops".into(),
        };
        assert!(err.to_string().contains("oops"));

        let err = StorageError::NotFound {
            store: StoreKind::PolicyCache,
            key: "missing".into(),
        };
        assert!(err.to_string().contains("missing"));

        let err = StorageError::SchemaVersionMismatch {
            expected: 1,
            actual: 2,
        };
        assert!(err.to_string().contains("1") && err.to_string().contains("2"));

        let err = StorageError::MigrationFailed {
            from: 1,
            to: 2,
            reason: "test".into(),
        };
        assert!(err.to_string().contains("test"));

        let err = StorageError::IntegrityViolation {
            store: StoreKind::ReplayIndex,
            detail: "corrupt".into(),
        };
        assert!(err.to_string().contains("corrupt"));

        let err = StorageError::BackendUnavailable {
            backend: "sqlite".into(),
            detail: "down".into(),
        };
        assert!(err.to_string().contains("sqlite") && err.to_string().contains("down"));

        let err = StorageError::WriteRejected {
            detail: "full".into(),
        };
        assert!(err.to_string().contains("full"));
    }

    #[test]
    fn storage_error_is_std_error() {
        let err = StorageError::NotFound {
            store: StoreKind::ReplayIndex,
            key: "k".into(),
        };
        let _: &dyn std::error::Error = &err;
    }

    // ── InMemoryStorageAdapter ───────────────────────────────────────────

    #[test]
    fn in_memory_ensure_schema_version_match() {
        let adapter = InMemoryStorageAdapter::new();
        assert!(
            adapter
                .ensure_schema_version(STORAGE_SCHEMA_VERSION)
                .is_ok()
        );
    }

    #[test]
    fn in_memory_ensure_schema_version_mismatch() {
        let adapter = InMemoryStorageAdapter::new();
        let err = adapter.ensure_schema_version(999).unwrap_err();
        assert!(
            matches!(err, StorageError::SchemaVersionMismatch { expected: 999, actual } if actual == STORAGE_SCHEMA_VERSION)
        );
    }

    #[test]
    fn in_memory_migrate_downgrade_rejected() {
        let mut adapter = InMemoryStorageAdapter::new();
        adapter.migrate_to(STORAGE_SCHEMA_VERSION + 1).unwrap();
        let err = adapter.migrate_to(STORAGE_SCHEMA_VERSION).unwrap_err();
        assert!(matches!(err, StorageError::MigrationFailed { .. }));
        assert!(err.to_string().contains("downgrade"));
    }

    #[test]
    fn in_memory_migrate_skip_version_rejected() {
        let mut adapter = InMemoryStorageAdapter::new();
        let err = adapter.migrate_to(STORAGE_SCHEMA_VERSION + 5).unwrap_err();
        assert!(matches!(err, StorageError::MigrationFailed { .. }));
        assert!(err.to_string().contains("single-step"));
    }

    #[test]
    fn in_memory_fail_writes_put_rejected() {
        let mut adapter = InMemoryStorageAdapter::new().with_fail_writes(true);
        let err = adapter
            .put(
                StoreKind::ReplayIndex,
                "k".to_string(),
                vec![1],
                BTreeMap::new(),
                &ctx(),
            )
            .unwrap_err();
        assert!(matches!(err, StorageError::WriteRejected { .. }));
    }

    #[test]
    fn in_memory_fail_writes_delete_rejected() {
        let mut adapter = InMemoryStorageAdapter::new().with_fail_writes(true);
        let err = adapter
            .delete(StoreKind::ReplayIndex, "k", &ctx())
            .unwrap_err();
        assert!(matches!(err, StorageError::WriteRejected { .. }));
    }

    #[test]
    fn in_memory_fail_writes_batch_rejected() {
        let mut adapter = InMemoryStorageAdapter::new().with_fail_writes(true);
        let entries = vec![BatchPutEntry {
            key: "k".to_string(),
            value: vec![1],
            metadata: BTreeMap::new(),
        }];
        let err = adapter
            .put_batch(StoreKind::ReplayIndex, entries, &ctx())
            .unwrap_err();
        assert!(matches!(err, StorageError::WriteRejected { .. }));
    }

    #[test]
    fn in_memory_query_limit_zero_errors() {
        let mut adapter = InMemoryStorageAdapter::new();
        let query = StoreQuery {
            limit: Some(0),
            ..Default::default()
        };
        let err = adapter
            .query(StoreKind::ReplayIndex, &query, &ctx())
            .unwrap_err();
        assert!(matches!(err, StorageError::InvalidQuery { .. }));
    }

    #[test]
    fn in_memory_query_with_key_prefix() {
        let mut adapter = InMemoryStorageAdapter::new();
        let context = ctx();
        adapter
            .put(
                StoreKind::ReplayIndex,
                "run/1".into(),
                vec![1],
                BTreeMap::new(),
                &context,
            )
            .unwrap();
        adapter
            .put(
                StoreKind::ReplayIndex,
                "run/2".into(),
                vec![2],
                BTreeMap::new(),
                &context,
            )
            .unwrap();
        adapter
            .put(
                StoreKind::ReplayIndex,
                "other/x".into(),
                vec![3],
                BTreeMap::new(),
                &context,
            )
            .unwrap();

        let query = StoreQuery {
            key_prefix: Some("run/".to_string()),
            ..Default::default()
        };
        let rows = adapter
            .query(StoreKind::ReplayIndex, &query, &context)
            .unwrap();
        assert_eq!(rows.len(), 2);
        assert!(rows.iter().all(|r| r.key.starts_with("run/")));
    }

    #[test]
    fn in_memory_query_with_metadata_filter() {
        let mut adapter = InMemoryStorageAdapter::new();
        let context = ctx();
        let mut meta = BTreeMap::new();
        meta.insert("env".to_string(), "prod".to_string());
        adapter
            .put(
                StoreKind::EvidenceIndex,
                "a".into(),
                vec![1],
                meta,
                &context,
            )
            .unwrap();
        adapter
            .put(
                StoreKind::EvidenceIndex,
                "b".into(),
                vec![2],
                BTreeMap::new(),
                &context,
            )
            .unwrap();

        let mut filters = BTreeMap::new();
        filters.insert("env".to_string(), "prod".to_string());
        let query = StoreQuery {
            metadata_filters: filters,
            ..Default::default()
        };
        let rows = adapter
            .query(StoreKind::EvidenceIndex, &query, &context)
            .unwrap();
        assert_eq!(rows.len(), 1);
        assert_eq!(rows[0].key, "a");
    }

    #[test]
    fn in_memory_query_with_limit() {
        let mut adapter = InMemoryStorageAdapter::new();
        let context = ctx();
        for i in 0..5 {
            adapter
                .put(
                    StoreKind::ReplayIndex,
                    format!("k/{i:03}"),
                    vec![i as u8],
                    BTreeMap::new(),
                    &context,
                )
                .unwrap();
        }
        let query = StoreQuery {
            limit: Some(2),
            ..Default::default()
        };
        let rows = adapter
            .query(StoreKind::ReplayIndex, &query, &context)
            .unwrap();
        assert_eq!(rows.len(), 2);
    }

    #[test]
    fn in_memory_get_nonexistent_returns_none() {
        let mut adapter = InMemoryStorageAdapter::new();
        let result = adapter
            .get(StoreKind::PolicyCache, "no-such-key", &ctx())
            .unwrap();
        assert!(result.is_none());
    }

    #[test]
    fn in_memory_delete_nonexistent_returns_false() {
        let mut adapter = InMemoryStorageAdapter::new();
        let deleted = adapter
            .delete(StoreKind::PolicyCache, "no-such-key", &ctx())
            .unwrap();
        assert!(!deleted);
    }

    #[test]
    fn in_memory_query_empty_store_returns_empty() {
        let mut adapter = InMemoryStorageAdapter::new();
        let rows = adapter
            .query(StoreKind::PlasWitness, &StoreQuery::default(), &ctx())
            .unwrap();
        assert!(rows.is_empty());
    }

    #[test]
    fn in_memory_put_updates_revision() {
        let mut adapter = InMemoryStorageAdapter::new();
        let context = ctx();
        let r1 = adapter
            .put(
                StoreKind::PolicyCache,
                "k".into(),
                vec![1],
                BTreeMap::new(),
                &context,
            )
            .unwrap();
        let r2 = adapter
            .put(
                StoreKind::PolicyCache,
                "k".into(),
                vec![2],
                BTreeMap::new(),
                &context,
            )
            .unwrap();
        assert!(r2.revision > r1.revision);
        assert_eq!(r2.value, vec![2]);
    }

    #[test]
    fn in_memory_backend_name() {
        let adapter = InMemoryStorageAdapter::new();
        assert_eq!(adapter.backend_name(), "in_memory");
    }

    // ── FrankensqliteStorageAdapter ──────────────────────────────────────

    #[derive(Debug, Default)]
    struct FailingBackend {
        fail_wal: bool,
        fail_pragma: bool,
        fail_schema_version: bool,
        fail_migrate: bool,
        fail_put: bool,
        fail_get: bool,
        fail_query: bool,
        fail_delete: bool,
        fail_batch: bool,
        inner: MockFrankenSqlite,
    }

    impl FrankensqliteBackend for FailingBackend {
        fn apply_wal_profile(&mut self) -> Result<(), String> {
            if self.fail_wal {
                Err("wal failure".into())
            } else {
                self.inner.apply_wal_profile()
            }
        }
        fn set_pragma(&mut self, key: &str, value: &str) -> Result<(), String> {
            if self.fail_pragma {
                Err("pragma failure".into())
            } else {
                self.inner.set_pragma(key, value)
            }
        }
        fn current_schema_version(&self) -> Result<u32, String> {
            if self.fail_schema_version {
                Err("schema version failure".into())
            } else {
                self.inner.current_schema_version()
            }
        }
        fn migrate_to(&mut self, target_version: u32) -> Result<(), String> {
            if self.fail_migrate {
                Err("migration failure".into())
            } else {
                self.inner.migrate_to(target_version)
            }
        }
        fn put_record(
            &mut self,
            store: StoreKind,
            key: &str,
            value: &[u8],
            metadata: &BTreeMap<String, String>,
        ) -> Result<StoreRecord, String> {
            if self.fail_put {
                Err("put failure".into())
            } else {
                self.inner.put_record(store, key, value, metadata)
            }
        }
        fn get_record(&self, store: StoreKind, key: &str) -> Result<Option<StoreRecord>, String> {
            if self.fail_get {
                Err("get failure".into())
            } else {
                self.inner.get_record(store, key)
            }
        }
        fn query_records(
            &self,
            store: StoreKind,
            query: &StoreQuery,
        ) -> Result<Vec<StoreRecord>, String> {
            if self.fail_query {
                Err("query failure".into())
            } else {
                self.inner.query_records(store, query)
            }
        }
        fn delete_record(&mut self, store: StoreKind, key: &str) -> Result<bool, String> {
            if self.fail_delete {
                Err("delete failure".into())
            } else {
                self.inner.delete_record(store, key)
            }
        }
        fn put_batch(
            &mut self,
            store: StoreKind,
            entries: &[BatchPutEntry],
        ) -> Result<Vec<StoreRecord>, String> {
            if self.fail_batch {
                Err("batch failure".into())
            } else {
                self.inner.put_batch(store, entries)
            }
        }
    }

    #[test]
    fn frankensqlite_new_wal_failure() {
        let backend = FailingBackend {
            fail_wal: true,
            ..Default::default()
        };
        let err = FrankensqliteStorageAdapter::new(backend).unwrap_err();
        assert!(matches!(err, StorageError::BackendUnavailable { .. }));
    }

    #[test]
    fn frankensqlite_new_pragma_failure() {
        let backend = FailingBackend {
            fail_pragma: true,
            ..Default::default()
        };
        let err = FrankensqliteStorageAdapter::new(backend).unwrap_err();
        assert!(matches!(err, StorageError::BackendUnavailable { .. }));
    }

    #[test]
    fn frankensqlite_new_schema_version_failure() {
        let backend = FailingBackend {
            fail_schema_version: true,
            ..Default::default()
        };
        let err = FrankensqliteStorageAdapter::new(backend).unwrap_err();
        assert!(matches!(err, StorageError::BackendUnavailable { .. }));
    }

    #[test]
    fn frankensqlite_crud_operations() {
        let backend = MockFrankenSqlite::default();
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let context = ctx();

        let record = adapter
            .put(
                StoreKind::ReplayIndex,
                "k1".into(),
                vec![1, 2],
                BTreeMap::new(),
                &context,
            )
            .unwrap();
        assert_eq!(record.key, "k1");
        assert_eq!(record.value, vec![1, 2]);

        let got = adapter.get(StoreKind::ReplayIndex, "k1", &context).unwrap();
        assert!(got.is_some());
        assert_eq!(got.unwrap().value, vec![1, 2]);

        let deleted = adapter
            .delete(StoreKind::ReplayIndex, "k1", &context)
            .unwrap();
        assert!(deleted);

        let got = adapter.get(StoreKind::ReplayIndex, "k1", &context).unwrap();
        assert!(got.is_none());
    }

    #[test]
    fn frankensqlite_query_limit_zero_errors() {
        let backend = MockFrankenSqlite::default();
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let query = StoreQuery {
            limit: Some(0),
            ..Default::default()
        };
        let err = adapter
            .query(StoreKind::ReplayIndex, &query, &ctx())
            .unwrap_err();
        assert!(matches!(err, StorageError::InvalidQuery { .. }));
    }

    #[test]
    fn frankensqlite_put_failure_emits_error_event() {
        let backend = FailingBackend {
            fail_put: true,
            ..Default::default()
        };
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let _ = adapter.put(
            StoreKind::ReplayIndex,
            "k".into(),
            vec![1],
            BTreeMap::new(),
            &ctx(),
        );
        let event = adapter.events().last().unwrap();
        assert_eq!(event.outcome, "error");
        assert!(event.error_code.is_some());
    }

    #[test]
    fn frankensqlite_get_failure() {
        let backend = FailingBackend {
            fail_get: true,
            ..Default::default()
        };
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let err = adapter
            .get(StoreKind::ReplayIndex, "k", &ctx())
            .unwrap_err();
        assert!(matches!(err, StorageError::BackendUnavailable { .. }));
    }

    #[test]
    fn frankensqlite_query_failure() {
        let backend = FailingBackend {
            fail_query: true,
            ..Default::default()
        };
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let err = adapter
            .query(StoreKind::ReplayIndex, &StoreQuery::default(), &ctx())
            .unwrap_err();
        assert!(matches!(err, StorageError::BackendUnavailable { .. }));
    }

    #[test]
    fn frankensqlite_delete_failure() {
        let backend = FailingBackend {
            fail_delete: true,
            ..Default::default()
        };
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let err = adapter
            .delete(StoreKind::ReplayIndex, "k", &ctx())
            .unwrap_err();
        assert!(matches!(err, StorageError::BackendUnavailable { .. }));
    }

    #[test]
    fn frankensqlite_batch_failure() {
        let backend = FailingBackend {
            fail_batch: true,
            ..Default::default()
        };
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let entries = vec![BatchPutEntry {
            key: "k".into(),
            value: vec![1],
            metadata: BTreeMap::new(),
        }];
        let err = adapter
            .put_batch(StoreKind::ReplayIndex, entries, &ctx())
            .unwrap_err();
        assert!(matches!(err, StorageError::BackendUnavailable { .. }));
    }

    #[test]
    fn frankensqlite_migrate_downgrade_rejected() {
        let backend = MockFrankenSqlite::default();
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        adapter.migrate_to(STORAGE_SCHEMA_VERSION + 1).unwrap();
        let err = adapter.migrate_to(STORAGE_SCHEMA_VERSION).unwrap_err();
        assert!(matches!(err, StorageError::MigrationFailed { .. }));
    }

    #[test]
    fn frankensqlite_migrate_skip_rejected() {
        let backend = MockFrankenSqlite::default();
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let err = adapter.migrate_to(STORAGE_SCHEMA_VERSION + 5).unwrap_err();
        assert!(matches!(err, StorageError::MigrationFailed { .. }));
    }

    #[test]
    fn frankensqlite_migrate_backend_failure() {
        let backend = FailingBackend {
            fail_migrate: true,
            ..Default::default()
        };
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let err = adapter.migrate_to(STORAGE_SCHEMA_VERSION + 1).unwrap_err();
        assert!(matches!(err, StorageError::BackendUnavailable { .. }));
    }

    #[test]
    fn frankensqlite_ensure_schema_version() {
        let backend = MockFrankenSqlite::default();
        let adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        assert!(
            adapter
                .ensure_schema_version(STORAGE_SCHEMA_VERSION)
                .is_ok()
        );
        let err = adapter.ensure_schema_version(999).unwrap_err();
        assert!(matches!(err, StorageError::SchemaVersionMismatch { .. }));
    }

    #[test]
    fn frankensqlite_backend_name() {
        let backend = MockFrankenSqlite::default();
        let adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        assert_eq!(adapter.backend_name(), "frankensqlite");
    }

    #[test]
    fn frankensqlite_batch_put_success() {
        let backend = MockFrankenSqlite::default();
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let entries = vec![
            BatchPutEntry {
                key: "a".into(),
                value: vec![1],
                metadata: BTreeMap::new(),
            },
            BatchPutEntry {
                key: "b".into(),
                value: vec![2],
                metadata: BTreeMap::new(),
            },
        ];
        let records = adapter
            .put_batch(StoreKind::ReplayIndex, entries, &ctx())
            .unwrap();
        assert_eq!(records.len(), 2);
    }

    #[test]
    fn frankensqlite_invalid_key_on_put() {
        let backend = MockFrankenSqlite::default();
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let err = adapter
            .put(
                StoreKind::ReplayIndex,
                "  ".into(),
                vec![1],
                BTreeMap::new(),
                &ctx(),
            )
            .unwrap_err();
        assert!(matches!(err, StorageError::InvalidKey { .. }));
    }

    #[test]
    fn frankensqlite_invalid_key_on_batch() {
        let backend = MockFrankenSqlite::default();
        let mut adapter = FrankensqliteStorageAdapter::new(backend).unwrap();
        let entries = vec![
            BatchPutEntry {
                key: "ok".into(),
                value: vec![1],
                metadata: BTreeMap::new(),
            },
            BatchPutEntry {
                key: "".into(),
                value: vec![2],
                metadata: BTreeMap::new(),
            },
        ];
        let err = adapter
            .put_batch(StoreKind::ReplayIndex, entries, &ctx())
            .unwrap_err();
        assert!(matches!(err, StorageError::InvalidKey { .. }));
    }

    // ── Utility functions ────────────────────────────────────────────────

    #[test]
    fn digest_hex_deterministic() {
        let a = digest_hex(b"hello");
        let b = digest_hex(b"hello");
        assert_eq!(a, b);
        assert_ne!(a, digest_hex(b"world"));
    }

    #[test]
    fn fnv1a64_deterministic() {
        let a = fnv1a64(b"test");
        let b = fnv1a64(b"test");
        assert_eq!(a, b);
    }

    #[test]
    fn fnv1a64_empty_is_offset_basis() {
        assert_eq!(fnv1a64(b""), 0xcbf2_9ce4_8422_2325);
    }

    #[test]
    fn canonicalize_records_sorts_by_key() {
        let r1 = StoreRecord {
            store: StoreKind::ReplayIndex,
            key: "z".into(),
            value: vec![1],
            metadata: BTreeMap::new(),
            revision: 1,
        };
        let r2 = StoreRecord {
            store: StoreKind::ReplayIndex,
            key: "a".into(),
            value: vec![2],
            metadata: BTreeMap::new(),
            revision: 2,
        };
        let sorted = canonicalize_records(vec![r1, r2], None);
        assert_eq!(sorted[0].key, "a");
        assert_eq!(sorted[1].key, "z");
    }

    #[test]
    fn canonicalize_records_with_limit() {
        let records: Vec<StoreRecord> = (0..5)
            .map(|i| StoreRecord {
                store: StoreKind::ReplayIndex,
                key: format!("k{i}"),
                value: vec![i as u8],
                metadata: BTreeMap::new(),
                revision: i as u64,
            })
            .collect();
        let truncated = canonicalize_records(records, Some(3));
        assert_eq!(truncated.len(), 3);
    }

    // ── Serde round-trips ────────────────────────────────────────────────

    #[test]
    fn store_record_serde_round_trip() {
        let record = StoreRecord {
            store: StoreKind::EvidenceIndex,
            key: "test".into(),
            value: vec![1, 2, 3],
            metadata: BTreeMap::new(),
            revision: 42,
        };
        let json = serde_json::to_string(&record).unwrap();
        let back: StoreRecord = serde_json::from_str(&json).unwrap();
        assert_eq!(back, record);
    }

    #[test]
    fn storage_event_serde_round_trip() {
        let event = StorageEvent {
            trace_id: "t".into(),
            decision_id: "d".into(),
            policy_id: "p".into(),
            component: "c".into(),
            event: "put".into(),
            outcome: "ok".into(),
            error_code: None,
        };
        let json = serde_json::to_string(&event).unwrap();
        let back: StorageEvent = serde_json::from_str(&json).unwrap();
        assert_eq!(back, event);
    }

    #[test]
    fn migration_receipt_serde_round_trip() {
        let receipt = MigrationReceipt {
            backend: "in_memory".into(),
            from_version: 1,
            to_version: 2,
            stores_touched: vec![StoreKind::ReplayIndex],
            records_touched: 10,
            state_hash_before: "aabb".into(),
            state_hash_after: "ccdd".into(),
        };
        let json = serde_json::to_string(&receipt).unwrap();
        let back: MigrationReceipt = serde_json::from_str(&json).unwrap();
        assert_eq!(back, receipt);
    }

    #[test]
    fn batch_put_entry_serde_round_trip() {
        let entry = BatchPutEntry {
            key: "k".into(),
            value: vec![1],
            metadata: BTreeMap::new(),
        };
        let json = serde_json::to_string(&entry).unwrap();
        let back: BatchPutEntry = serde_json::from_str(&json).unwrap();
        assert_eq!(back, entry);
    }

    #[test]
    fn store_query_default() {
        let q = StoreQuery::default();
        assert!(q.key_prefix.is_none());
        assert!(q.metadata_filters.is_empty());
        assert!(q.limit.is_none());
    }

    #[test]
    fn in_memory_events_record_success_and_failure() {
        let mut adapter = InMemoryStorageAdapter::new();
        let context = ctx();
        adapter
            .put(
                StoreKind::ReplayIndex,
                "valid".into(),
                vec![1],
                BTreeMap::new(),
                &context,
            )
            .unwrap();
        let _ = adapter.put(
            StoreKind::ReplayIndex,
            "".into(),
            vec![1],
            BTreeMap::new(),
            &context,
        );

        let events = adapter.events();
        assert_eq!(events.len(), 2);
        assert_eq!(events[0].outcome, "ok");
        assert_eq!(events[1].outcome, "error");
    }
}
